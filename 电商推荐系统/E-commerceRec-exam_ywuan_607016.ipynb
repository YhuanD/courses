{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线电商推荐系统项目特训营结业考试\n",
    "#### 考试说明:\n",
    "- 起止时间：<font color=red><b>2020年2月28日10:00 - 2020年2月29日00:00</b></font>，<b>逾期不接受补考</b>\n",
    "- 考试提交方式：<font color=red><b> 将试卷下载到本地作答</b></font>，本地作答完成后将文件命名为<font color=red><b>\"姓名_学号\"</b></font>并上传到：http://47.93.208.249:9825/tree/0.Teacher/Exam/Stage1\n",
    "- 注意事项：为确保同学们真正了解自身对这次课程的掌握程度，<font color=red><b>请勿翻阅抄袭，移动，更改</b></font>其它同学的试卷，如发现按0分处理。\n",
    "- 请同学在下方同学姓名处 <font color=red><b>填写自己的姓名</b></font>，批改人和最终得分处不用填写，试卷总分为100分。\n",
    "- 考试用到的数据下载地址：http://47.93.208.249:9825/tree/0.Teacher/Exam ， 将数据 movies.dat 和 ratings.dat 放在与试卷同目录下使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:ywuan\n",
    "- 批改人：魏天保\n",
    "- 最终得分: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、理论题（40分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、协同过滤算法是指基于用户行为数据设计的推荐算法，常见的有UserCF和ItemCF两种，请你分别用一句话概括这两种算法的核心思想。（10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UserCF的核心思想是：给用户推荐和他兴趣相似的其他用户喜欢的物品。ItemCF的核心思想是：给用户推荐和其过去感兴趣的物品相似的物品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、 矩阵分解属于协同过滤算法吗？（5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 属于\n",
    "\n",
    "B 不属于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、请列举出基于内容的推荐的两个优点（6分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1、不需要用户的历史数据，如对对象的评价等。\n",
    "2、没有关于新推荐对象出现的冷启动问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、请说出推荐系统中评分预测和TopN推荐的评价指标，各列举两种。（4分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "评分预测：RMSE、MSE\n",
    "TopN推荐：Precision、Recall。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5、 写出RMSE的计算公式，并说明公式中每个变量的含义。（5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RMSE为均方根误差，公式为sqrt(mean((y_r-y_p)^2)),y_r为实际值，y_p为预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6、Deep AutoEncoder 的核心思想是什么。（10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "核心的思想是希望能通过反向传播训练网络最小化输入和输出的误差，从而完成对未知数据的预估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、代码题（60分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7、请按要求完成基于 tensorflow 的 Deep AutoEncoder 算法。（共30分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ratings.dat', sep='\\t', names=['user', 'item', 'rating', 'timestamp'], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 去掉评分数据 df 中的 “timestamp” 这一列，并输出 df 前五行（5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0     1  1193       5\n",
       "1     1   661       3\n",
       "2     1   914       3\n",
       "3     1  3408       4\n",
       "4     1  2355       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['timestamp']\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看用户数和电影数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 6040 ITEMS: 3706\n"
     ]
    }
   ],
   "source": [
    "num_items = df.item.nunique()\n",
    "num_users = df.user.nunique()\n",
    "print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 对 df 的 “rating” 列做 Normalization，展示 df 前 5 行（5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0     1  1193    1.00\n",
       "1     1   661    0.50\n",
       "2     1   914    0.50\n",
       "3     1  3408    0.75\n",
       "4     1  2355    1.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "r = df['rating'].values.astype('float')\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['rating'] = df_normalized\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 把 df 转成 user-item 矩阵，存储在变量 matrix 中 ，用 0 填充矩阵的缺失值，从matrix中取出user5对item6的评分值（5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = df.pivot(index='user',columns='item',values='rating')\n",
    "matrix.fillna(0,inplace=True)\n",
    "matrix.loc[5,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取用户和物品列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = matrix.index.tolist()\n",
    "items = matrix.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将 matrix 从 dataframe 转换成 numpy 数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = num_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 隐层的变量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 构建 encoder 和 decoder （5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 =  tf.nn.relu(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 =  tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建整个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测 y 值和真实 y 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义评估准则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 变量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 session 中 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 149.51889538764954\n",
      "Epoch: 2 Loss: 5.1992259820302325\n",
      "Epoch: 3 Loss: 1.4357259174187977\n",
      "Epoch: 4 Loss: 0.6283764677743117\n",
      "Epoch: 5 Loss: 0.4077149343987306\n",
      "Epoch: 6 Loss: 0.26733827715118724\n",
      "Epoch: 7 Loss: 0.11386655705670516\n",
      "Epoch: 8 Loss: 0.03647922220019003\n",
      "Epoch: 9 Loss: 0.021631047207241256\n",
      "Epoch: 10 Loss: 0.020615851002124447\n",
      "Epoch: 11 Loss: 0.02061576085786025\n",
      "Epoch: 12 Loss: 0.02062916538367669\n",
      "Epoch: 13 Loss: 0.02062787883915007\n",
      "Epoch: 14 Loss: 0.020616445147121947\n",
      "Epoch: 15 Loss: 0.02061628255372246\n",
      "Epoch: 16 Loss: 0.02060810449377944\n",
      "Epoch: 17 Loss: 0.02061281605468442\n",
      "Epoch: 18 Loss: 0.020610111687953275\n",
      "Epoch: 19 Loss: 0.020614335623880226\n",
      "Epoch: 20 Loss: 0.020608811639249325\n",
      "Epoch: 21 Loss: 0.020604100738031168\n",
      "Epoch: 22 Loss: 0.020605522169110674\n",
      "Epoch: 23 Loss: 0.020600884221494198\n",
      "Epoch: 24 Loss: 0.020605744832816224\n",
      "Epoch: 25 Loss: 0.020602707867510617\n",
      "Epoch: 26 Loss: 0.020600740021715563\n",
      "Epoch: 27 Loss: 0.020593449977847438\n",
      "Epoch: 28 Loss: 0.020599197751532\n",
      "Epoch: 29 Loss: 0.02060457846770684\n",
      "Epoch: 30 Loss: 0.020596322681133945\n",
      "Epoch: 31 Loss: 0.020599979635638494\n",
      "Epoch: 32 Loss: 0.020591460828048486\n",
      "Epoch: 33 Loss: 0.020590282084109884\n",
      "Epoch: 34 Loss: 0.020593864959664643\n",
      "Epoch: 35 Loss: 0.020594604313373566\n",
      "Epoch: 36 Loss: 0.020588744160098333\n",
      "Epoch: 37 Loss: 0.020590742390292387\n",
      "Epoch: 38 Loss: 0.020594898494891822\n",
      "Epoch: 39 Loss: 0.020591509334432583\n",
      "Epoch: 40 Loss: 0.020585835988943774\n",
      "Epoch: 41 Loss: 0.020589118512968223\n",
      "Epoch: 42 Loss: 0.020589366283578176\n",
      "Epoch: 43 Loss: 0.020586826450501878\n",
      "Epoch: 44 Loss: 0.020586284653594095\n",
      "Epoch: 45 Loss: 0.020582196457932394\n",
      "Epoch: 46 Loss: 0.020585061710638303\n",
      "Epoch: 47 Loss: 0.020578754250891507\n",
      "Epoch: 48 Loss: 0.020584028175411124\n",
      "Epoch: 49 Loss: 0.020577486255206168\n",
      "Epoch: 50 Loss: 0.020580827607773244\n",
      "Epoch: 51 Loss: 0.020580389032450814\n",
      "Epoch: 52 Loss: 0.020582235728700955\n",
      "Epoch: 53 Loss: 0.02058143203612417\n",
      "Epoch: 54 Loss: 0.0205756234160314\n",
      "Epoch: 55 Loss: 0.02058070032702138\n",
      "Epoch: 56 Loss: 0.020580113943045337\n",
      "Epoch: 57 Loss: 0.020572884667975206\n",
      "Epoch: 58 Loss: 0.020570582360960543\n",
      "Epoch: 59 Loss: 0.02057547211491813\n",
      "Epoch: 60 Loss: 0.02057803173859914\n",
      "Epoch: 61 Loss: 0.020572464214637876\n",
      "Epoch: 62 Loss: 0.02057479399566849\n",
      "Epoch: 63 Loss: 0.02057410132450362\n",
      "Epoch: 64 Loss: 0.02057114561709265\n",
      "Epoch: 65 Loss: 0.02056917272663365\n",
      "Epoch: 66 Loss: 0.020568168993728857\n",
      "Epoch: 67 Loss: 0.020570491789840162\n",
      "Epoch: 68 Loss: 0.02057452038085709\n",
      "Epoch: 69 Loss: 0.0205693068758895\n",
      "Epoch: 70 Loss: 0.020565041069251794\n",
      "Epoch: 71 Loss: 0.020573159912601113\n",
      "Epoch: 72 Loss: 0.020568237213107448\n",
      "Epoch: 73 Loss: 0.020562176670258243\n",
      "Epoch: 74 Loss: 0.020562665342974167\n",
      "Epoch: 75 Loss: 0.020568582811392844\n",
      "Epoch: 76 Loss: 0.02055978305482616\n",
      "Epoch: 77 Loss: 0.020563623441072803\n",
      "Epoch: 78 Loss: 0.02056601899676025\n",
      "Epoch: 79 Loss: 0.020564764350031812\n",
      "Epoch: 80 Loss: 0.02056646327643345\n",
      "Epoch: 81 Loss: 0.020563260613319773\n",
      "Epoch: 82 Loss: 0.020563479396514595\n",
      "Epoch: 83 Loss: 0.020565815231141944\n",
      "Epoch: 84 Loss: 0.0205647546875601\n",
      "Epoch: 85 Loss: 0.02055786243484666\n",
      "Epoch: 86 Loss: 0.02056227597252776\n",
      "Epoch: 87 Loss: 0.020561728820515175\n",
      "Epoch: 88 Loss: 0.020557818429855008\n",
      "Epoch: 89 Loss: 0.02056077215820551\n",
      "Epoch: 90 Loss: 0.020555421127937734\n",
      "Epoch: 91 Loss: 0.02055833601237585\n",
      "Epoch: 92 Loss: 0.020555145883311827\n",
      "Epoch: 93 Loss: 0.020558599072198074\n",
      "Epoch: 94 Loss: 0.020561613840982318\n",
      "Epoch: 95 Loss: 0.020558149670250714\n",
      "Epoch: 96 Loss: 0.02055816996532182\n",
      "Epoch: 97 Loss: 0.020556870731525123\n",
      "Epoch: 98 Loss: 0.02055997125959645\n",
      "Epoch: 99 Loss: 0.020559549797326326\n",
      "Epoch: 100 Loss: 0.020553065075849492\n",
      "Predictions...\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame()\n",
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 250\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='rating')\n",
    "    predictions.columns = ['user', 'item', 'rating']\n",
    "    predictions['user'] = predictions['user'].map(lambda value: users[value])\n",
    "    predictions['item'] = predictions['item'].map(lambda value: items[value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 为用户42计算top10的推荐结果（10分）\n",
    "- top10 的筛选依据是评分预测最高的 top10\n",
    "- 要求推荐的 top10 是该用户没有看过的电影\n",
    "- 最后输出推荐的 top10 电影的 itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153431    1617\n",
       "151946       1\n",
       "151995      50\n",
       "152798     913\n",
       "153005    1136\n",
       "153101    1247\n",
       "153113    1259\n",
       "154721    2987\n",
       "153085    1230\n",
       "154532    2791\n",
       "Name: item, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_42p = predictions[predictions.user==42]\n",
    "df_42r = df[df.user==42]\n",
    "df_42 = df_42p.append(df_42r)\n",
    "df_42.drop_duplicates(subset=['item'],keep=False,inplace=True)\n",
    "df_42.sort_values(by='rating',ascending=False)['item'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8、请你实现基于Keras的协同深度学习算法。（共30分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input, Embedding, Dot, Flatten, Dense, Dropout, Lambda, Add\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.initializers import RandomUniform, RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 请你完成 movie_map 这个函数（5分）\n",
    "- 功能：使用自然数对 movies.dat 中的 movieid 进行重编码\n",
    "- 返回值：movieid 到编码 id 的字典\n",
    "- 测试：计算编码字典，存入变量 d（后面有用），输出 d[520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def movie_map(file='movies.dat'):\n",
    "    movies = read_csv(file, sep='::', header=None)\n",
    "    value = movies[0].unique()\n",
    "    index = range(movies[0].nunique())\n",
    "    return dict(zip(value,index))\n",
    "                \n",
    "d = movie_map()\n",
    "d[520]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 请你完成 read_ratings 这个函数（5分）\n",
    "- 要求返回数据类型为 float32 的二维 numpy 数组：每一行表示用户评分记录 [user,item,rating]\n",
    "- 原始数据中的 itemid 需要使用编码字典 d 进行重编码\n",
    "- rating 值需要进行幅度缩放，缩放区间为[0,1]\n",
    "- 测试：调用 read_ratings ,将返回值存入变量 ratings （后面有用），并输出 ratings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 1.176e+03, 1.000e+00],\n",
       "       [1.000e+00, 6.550e+02, 5.000e-01],\n",
       "       [1.000e+00, 9.020e+02, 5.000e-01]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_ratings(file='ratings.dat'):\n",
    "   \n",
    "    rating_mat = list()\n",
    "    with open(file) as fp:\n",
    "        for line in fp:\n",
    "            line = line.strip().split('\\t')\n",
    "            user, item, rating = line[0], d[int(line[1])], line[2]\n",
    "            rating_mat.append( [user, item, rating] )\n",
    "    min_r = int(min([x[2] for x in rating_mat]))\n",
    "    max_r = int(max([x[2] for x in rating_mat]))\n",
    "    for i in range(len(rating_mat)):\n",
    "        rating_mat[i][2] = (int(rating_mat[i][2])-min_r)/(max_r-min_r)\n",
    "    return np.array(rating_mat).astype('float32')\n",
    "\n",
    "ratings = read_ratings()\n",
    "ratings[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 请你完成 train_test_split 这个函数（5分）\n",
    "- 功能：将评分数据 ratings 划分为训练集和测试集两部分\n",
    "- 输入：函数 read_ratings 的返回值 ratings\n",
    "- 输出：训练集 train_mat，测试集 test_mat\n",
    "- 要求：训练集占 80%，测试集占 20%\n",
    "- 测试：调用该函数，返回值存入 train_mat, test_mat （后面有用），并展示训练集和测试集的样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800167, 200042)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(ratings):\n",
    "    num_train = int(len(ratings)*0.8)\n",
    "    train_mat = ratings[:num_train]\n",
    "    test_mat = ratings[num_train:]\n",
    "    return train_mat,test_mat\n",
    "\n",
    "train_mat, test_mat = train_test_split(ratings)\n",
    "train_mat.shape[0], test_mat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 请你完成 read_item 这个函数（5分）\n",
    "- 功能：将 movies.dat 的三列特征进行重编码\n",
    "- 要求：\n",
    "    - 第 1 列特征使用编码字典 d 进行编码\n",
    "    - 第 2、3 列特征使用 LabelEncoder() 编码\n",
    "    - 返回值有两个，第一个是二维 numpy 数组 ，也就是一个矩阵：每一行对应原始数据 movies.dat 的一行记录；第二个是特征维度，也就是矩阵的列数\n",
    "    - 调用该函数，返回值存入 item_mat,item_feat_dim （后面有用），打印 item_mat[:3] 和 item_feat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 3574  145]\n",
      " [   1 1858  115]\n",
      " [   2 1483  207]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def read_item(file='movies.dat'):\n",
    "    \n",
    "    item = read_csv(file, sep='::', header=None)\n",
    "    item[0] = item[0].apply(lambda x:d[int(x)])\n",
    "    for i in range(1,3):\n",
    "        item[i] = LabelEncoder().fit_transform(item[i])\n",
    "    return np.array(item),item.shape[1]\n",
    "\n",
    "item_mat,item_feat_dim = read_item()\n",
    "print(item_mat[:3])\n",
    "print(item_feat_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CollaborativeDeepLearning 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeDeepLearning:\n",
    "    \n",
    "    def __init__(self, item_mat, hidden_layers):\n",
    "        '''\n",
    "        hidden_layers = a list of three integer indicating the embedding dimension of autoencoder\n",
    "        item_mat = item feature matrix with shape (# of item, # of item features)\n",
    "        '''\n",
    "        assert(len(hidden_layers)==3)\n",
    "        self.item_mat = item_mat\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.item_dim = hidden_layers[0]\n",
    "        self.embedding_dim = hidden_layers[-1]\n",
    "        \n",
    "    def pretrain(self, lamda_w=0.1, encoder_noise=0.1, dropout_rate=0.1, activation='sigmoid', batch_size=64, epochs=10):\n",
    "        '''\n",
    "        layer-wise pretraining on item features (item_mat)\n",
    "        '''\n",
    "        self.trained_encoders = []\n",
    "        self.trained_decoders = []\n",
    "        X_train = self.item_mat\n",
    "        \n",
    "        for input_dim, hidden_dim in zip(self.hidden_layers[:-1], self.hidden_layers[1:]):\n",
    "            \n",
    "            pretrain_input = Input(shape=(input_dim,))\n",
    "            encoded = GaussianNoise(stddev=encoder_noise)(pretrain_input)\n",
    "            encoded = Dropout(dropout_rate)(encoded)\n",
    "            encoder = Dense(hidden_dim, activation=activation, kernel_regularizer=l2(lamda_w), bias_regularizer=l2(lamda_w))(encoded)\n",
    "            decoder = Dense(input_dim, activation=activation, kernel_regularizer=l2(lamda_w), bias_regularizer=l2(lamda_w))(encoder)\n",
    "            \n",
    "            # autoencoder\n",
    "            ae = Model(inputs=pretrain_input, outputs=decoder)\n",
    "            \n",
    "            # encoder\n",
    "            ae_encoder = Model(inputs=pretrain_input, outputs=encoder)\n",
    "            \n",
    "            # decoder\n",
    "            encoded_input = Input(shape=(hidden_dim,))\n",
    "            \n",
    "            decoder_layer = ae.layers[-1] # the last layer\n",
    "            ae_decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "            ae.compile(loss='mse', optimizer='rmsprop')\n",
    "            ae.fit(X_train, X_train, batch_size=batch_size, epochs=epochs, verbose=2)\n",
    "\n",
    "            self.trained_encoders.append(ae_encoder)\n",
    "            self.trained_decoders.append(ae_decoder)\n",
    "            X_train = ae_encoder.predict(X_train)\n",
    "\n",
    "    def fineture(self, train_mat, test_mat, lamda_u=0.1, lamda_v=0.1, lamda_n=0.1, lr=0.001, batch_size=64, epochs=10):\n",
    "        '''\n",
    "        Fine-tuning with rating prediction\n",
    "        '''\n",
    "        num_user = int( max(train_mat[:,0].max(), test_mat[:,0].max()) + 1 )\n",
    "        num_item = int( max(train_mat[:,1].max(), test_mat[:,1].max()) + 1 )\n",
    "\n",
    "        # item autoencoder \n",
    "        itemfeat_InputLayer = Input(shape=(self.item_dim,), name='item_feat_input')\n",
    "        encoded = self.trained_encoders[0](itemfeat_InputLayer)\n",
    "        encoded = self.trained_encoders[1](encoded)\n",
    "        decoded = self.trained_decoders[1](encoded)\n",
    "        decoded = self.trained_decoders[0](decoded)\n",
    "\n",
    "        # user embedding\n",
    "        user_InputLayer = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "        user_EmbeddingLayer = Embedding(input_dim=num_user, output_dim=self.embedding_dim,\n",
    "                                        input_length=1, name='user_embedding', \n",
    "                                        embeddings_regularizer=l2(lamda_u),\n",
    "                                        embeddings_initializer=RandomNormal(mean=0, stddev=1))(user_InputLayer)\n",
    "        user_EmbeddingLayer = Flatten(name='user_flatten')(user_EmbeddingLayer)\n",
    "\n",
    "        # item embedding\n",
    "        item_InputLayer = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "        item_OffsetVector = Embedding(input_dim=num_item, output_dim=self.embedding_dim, \n",
    "                                      input_length=1, name='item_offset_vector',\n",
    "                                      embeddings_regularizer=l2(lamda_v), \n",
    "                                      embeddings_initializer=RandomNormal(mean=0, stddev=1))(item_InputLayer)\n",
    "        item_OffsetVector = Flatten(name='item_flatten')(item_OffsetVector)\n",
    "        item_EmbeddingLayer = Add()([encoded, item_OffsetVector]) \n",
    "        \n",
    "        # rating prediction\n",
    "        dotLayer = Dot(axes = -1, name='dot_layer')([user_EmbeddingLayer, item_EmbeddingLayer])\n",
    "\n",
    "        self.cdl_model = Model(inputs=[user_InputLayer, item_InputLayer, itemfeat_InputLayer], outputs=[dotLayer, decoded])\n",
    "        self.cdl_model.compile(optimizer='rmsprop', loss=['mse', 'mse'], loss_weights=[1, lamda_n])\n",
    "\n",
    "        train_user, train_item, train_item_feat, train_label = self.matrix2input(train_mat)\n",
    "        test_user, test_item, test_item_feat, test_label = self.matrix2input(test_mat)\n",
    "\n",
    "        model_history = self.cdl_model.fit([train_user, train_item, train_item_feat], \n",
    "                                           [train_label, train_item_feat], \n",
    "                                           epochs=epochs, batch_size=batch_size, \n",
    "                                           validation_data=([test_user, test_item, test_item_feat], [test_label, test_item_feat]))\n",
    "        return model_history\n",
    "\n",
    "    def matrix2input(self, rating_mat):\n",
    "        train_user = rating_mat[:, 0].reshape(-1, 1).astype(int)\n",
    "        train_item = rating_mat[:, 1].reshape(-1, 1).astype(int)\n",
    "        train_label = rating_mat[:, 2].reshape(-1, 1)\n",
    "        train_item_feat = [self.item_mat[train_item[x]][0] for x in range(train_item.shape[0])]\n",
    "        return train_user, train_item, np.array(train_item_feat), train_label\n",
    "    \n",
    "    def build(self):\n",
    "        # rating prediction\n",
    "        prediction_layer = Dot(axes = -1, name='prediction_layer')([user_EmbeddingLayer, encoded])\n",
    "        self.model = Model(inputs=[user_InputLayer, itemfeat_InputLayer], outputs=[prediction_layer])\n",
    "        \n",
    "    def getRMSE(self, test_mat):\n",
    "        test_user, test_item, test_item_feat, test_label = self.matrix2input(test_mat)\n",
    "        pred_out = self.cdl_model.predict([test_user, test_item, test_item_feat])\n",
    "        return np.sqrt(np.mean(np.square(test_label.flatten() - pred_out[0].flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 成功运行下面的代码，输出RMSE（10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 3362810.8983\n",
      "Epoch 2/10\n",
      " - 0s - loss: 3362529.0643\n",
      "Epoch 3/10\n",
      " - 0s - loss: 3362253.1535\n",
      "Epoch 4/10\n",
      " - 0s - loss: 3362099.5319\n",
      "Epoch 5/10\n",
      " - 0s - loss: 3361980.8258\n",
      "Epoch 6/10\n",
      " - 0s - loss: 3361893.0852\n",
      "Epoch 7/10\n",
      " - 0s - loss: 3361823.1601\n",
      "Epoch 8/10\n",
      " - 0s - loss: 3361759.6746\n",
      "Epoch 9/10\n",
      " - 0s - loss: 3361716.1151\n",
      "Epoch 10/10\n",
      " - 0s - loss: 3361683.5159\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2509\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1983\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1598\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.1334\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1166\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.1066\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.1010\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0976\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0954\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0939\n",
      "Train on 800167 samples, validate on 200042 samples\n",
      "Epoch 1/4\n",
      "800167/800167 [==============================] - 38s 47us/step - loss: 320009.9658 - dot_layer_loss: 0.6862 - model_3_loss: 3198583.5481 - val_loss: 311579.5032 - val_dot_layer_loss: 0.5071 - val_model_3_loss: 3115789.5770\n",
      "Epoch 2/4\n",
      "800167/800167 [==============================] - 40s 49us/step - loss: 319858.8131 - dot_layer_loss: 0.4293 - model_3_loss: 3198583.4605 - val_loss: 311579.5034 - val_dot_layer_loss: 0.5071 - val_model_3_loss: 3115789.5770\n",
      "Epoch 3/4\n",
      "800167/800167 [==============================] - 41s 52us/step - loss: 319858.8131 - dot_layer_loss: 0.4293 - model_3_loss: 3198583.4627 - val_loss: 311579.5033 - val_dot_layer_loss: 0.5071 - val_model_3_loss: 3115789.5770\n",
      "Epoch 4/4\n",
      "800167/800167 [==============================] - 42s 53us/step - loss: 319858.8128 - dot_layer_loss: 0.4292 - model_3_loss: 3198583.4605 - val_loss: 311579.5034 - val_dot_layer_loss: 0.5071 - val_model_3_loss: 3115789.5770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71212685"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Run\n",
    "model = CollaborativeDeepLearning(item_mat, [item_feat_dim, 16, 8])\n",
    "model.pretrain(lamda_w=0.001, encoder_noise=0.3, epochs=10)\n",
    "model_history = model.fineture(train_mat, test_mat, lamda_u=0.01, lamda_v=0.1, lamda_n=0.1, lr=0.01, epochs=4)\n",
    "model.getRMSE(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
