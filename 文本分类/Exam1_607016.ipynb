{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考试文件，需要请下载download到本地，作答完成后，再上传到云平台students文件夹内即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、用jieba分词库和停用词表对下面句子进行分词并去除停用词？（停用词表可以用课上用的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "sentence = \" \\u3000\\u30002016年是综   艺井喷的一年，《2016年中国网络视听发展研究报告》数据显示，截至2016年11月30日，视频网站备案的网络综艺为618档，6637期。2016年可谓是当之无愧的网络综艺黄金年。“爆款”网综《奇葩说》的第三季继续延续了火热，《火星情报局》、《拜托了冰箱》等网络综艺也都获得了较高的点击率与不俗的口碑，然而这些网络综艺距“现象级”都还有一定的差距。 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = jieba.lcut(sentence)\n",
    "stopwords=pd.read_csv(\"NLP_project/data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "result = [i for i in seg if i not in stopwords.stopword.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '综',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '艺',\n",
       " '井喷',\n",
       " '一年',\n",
       " '中国',\n",
       " '网络',\n",
       " '视听',\n",
       " '发展',\n",
       " '研究',\n",
       " '报告',\n",
       " '数据',\n",
       " '显示',\n",
       " '视频',\n",
       " '网站',\n",
       " '备案',\n",
       " '网络',\n",
       " '综艺',\n",
       " '618',\n",
       " '档',\n",
       " '6637',\n",
       " '期',\n",
       " '可谓',\n",
       " '当之无愧',\n",
       " '网络',\n",
       " '综艺',\n",
       " '黄金',\n",
       " '爆款',\n",
       " '网综',\n",
       " '奇葩',\n",
       " '说',\n",
       " '第三季',\n",
       " '延续',\n",
       " '火热',\n",
       " '火星',\n",
       " '情报局',\n",
       " '拜托',\n",
       " '冰箱',\n",
       " '网络',\n",
       " '综艺',\n",
       " '高',\n",
       " '点击率',\n",
       " '不俗',\n",
       " '口碑',\n",
       " '网络',\n",
       " '综艺',\n",
       " '现象',\n",
       " '级',\n",
       " '差距',\n",
       " ' ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、计算TFIDF值？\n",
    "写下TFIDF计算公式，并手动或调库计算单词 \"first\" 的TFIDF值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['this is the first document.',\n",
    "          'this is the  second document.',\n",
    "          'and the third one.',\n",
    "          'first the third document?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案：\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)\n",
    "\n",
    "$TF-IDF=TF∗IDF$\n",
    "\n",
    "$TF_W = \\frac{在某一类中词条w出现的次数}{该类中所有的词条数目}$\n",
    "\n",
    "$IDF = log(\\frac{语料库的文档总数}{包含词条w的文档数+1})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "first in 'this is the first document.':  0.49425506218038123\n",
      "first in 'first the third document?':  0.5685556582078485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "print(tfidf_vec.get_feature_names())\n",
    "print(tfidf_vec.vocabulary_)\n",
    "num = tfidf_vec.vocabulary_['first']\n",
    "#print(tfidf_matrix.toarray())\n",
    "print(\"first in 'this is the first document.': \",tfidf_matrix.toarray()[0][num])\n",
    "print(\"first in 'first the third document?': \",tfidf_matrix.toarray()[3][num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、朴素贝叶斯完成中文文本分类课程中，除了贝叶斯和SVM，请换一个模型完成，代码和测试结果粘贴在下面？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df_technology = pd.read_csv(\"NLP_project/data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"NLP_project/data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"NLP_project/data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"NLP_project/data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"NLP_project/data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)\n",
    "            segs = filter(lambda x:x not in stopwords, segs)\n",
    "            sentences.append((\" \".join(segs), category))\n",
    "        except Exception:\n",
    "            print (line)\n",
    "            continue \n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65696"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=RandomForestClassifier(n_estimators=8)):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), max_features=20000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.8088040549796794\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、Fasttext相比于word2vec做了什么改进？（面试高频）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案：\n",
    "\n",
    "改善运算效率：fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。fastText 也利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。因此，频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高。\n",
    "\n",
    "fasttext有别于word2vec的另一点是加了ngram切分，将长词再通过ngram切分为几个短词，这样对于未登录词也可以通过切出来的ngram词向量合并为一个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、对于用CNN类模型作文本分类和RNN类模型做文本分类，你觉得各自的优势和特点是什么，分别适合什么场景？（面试高频）\n",
    "\n",
    "答案：cnn善于捕捉局部信息，RNN擅长处理时序信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
